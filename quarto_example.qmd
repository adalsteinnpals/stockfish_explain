---
title: "Stockfish Explainability"
format:
  html:
    code-fold: true
jupyter: python3
toc: true
number-sections: true
---

# Explainability and Stockfish

## Introduction
This document is a work in progress. It is intended to be a guide to the explainability features of Stockfish. It is not intended to be a guide to Stockfish itself. For that, please see the [Stockfish documentation](https://stockfishchess.org/).

## What is Explainability?
Explainability is the ability to understand why a machine learning model makes the decisions it does. It is a key component of the Explainable AI movement. It is also a key component of the Fairness, Accountability, and Transparency

## Explainability Methods
In the quest to understand why a machine learning model makes the decision it does we want to find out what it has learnt. The methods we will be using to explain Stockfish are:

- Feature Importance
- Saliency Maps
- Concept Probing  







# Concept Probing




```{python}
#| label: fig-polar
#| fig-cap: "A line plot on a polar axis"

import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```


# Next Chapter