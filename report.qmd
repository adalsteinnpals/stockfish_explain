---
title: "Stockfish Explainability"
format:
  html:
    code-fold: true
jupyter: python3
bibliography: references.bib
toc: true
number-sections: true
---

# Explainability and Stockfish

## Introduction
This document is a work in progress. It is intended to be a guide to the explainability features of Stockfish. It is not intended to be a guide to Stockfish itself. For that, please see the [@Stockfish] documentation.

Meeting notes can be found [here](notes.qmd)

## What is Explainability?
Explainability is the ability to understand why a machine learning model makes the decisions it does. It is a key component of the Explainable AI movement. It is also a key component of the Fairness, Accountability, and Transparency

## Explainability Methods
In the quest to understand why a machine learning model makes the decision it does we want to find out what it has learnt. The methods we will be using to explain Stockfish are:

- Feature Importance
- Saliency Maps
- Concept Probing  



# Chess

Notes about chess concepts can be found [here](chess_concepts.qmd)

## Concepts from Stockfish

- **Material**
- **Imbalance**
- **Pawns**
- **Knights**
- **Bishops**
- **Rooks**
- **Queens**
- **Mobility**
- **King Safety**
- **Threats**
- **Passed Pawns**
- **Space**
- **Winnable**

## Implemented Concepts
All concepts are implemented for both white and black.

- **white_bishop_pair** - Is true if white has a bishop pair
- **white_knight_pair** - Is true if white has a knight pair
- **white_double_pawn** - Is true if white has a double pawn
- **white_isolated_pawn** - Is true if white has an isolated pawn
- **white_connected_rooks** - Is true if white has connected rooks
- **white_rook_on_open_file** - Is true if white has a rook on an open file
- **has_contested_open_file** - Is true if there is a contested open file
- **current_player_is_forking** - Is true if the current player is forking
- **current_player_can_fork** - Is true if the current player can fork
- **current_player_is_checking** - Is true if the current player is checking
- **current_player_can_check** - Is true if the current player can check
- **material imbalance** - The material imbalance




# Concept Probing



In this chapter we will compare various probing methods:

- Ridge Classifier
- LightGBM

Methods suggested in [@DBLP:journals/corr/abs-2111-09259]:

- data-dependent sparcity using Gated Linear Networks
- information-theoretic regularization using the information bottleneck
- minimum description length probing
- Bayesian probing 


## Minimum description length probing

The accuracy of a probe is commonly used as a proxy of how well an property is encoded by a model. To show a difference between two properties, often ether the amount of training data or the probing model (size) is restricted. 

Instead this paper presents an approach that approximates the "amount of effort" needed to achieve a given probing accuracy. High accuracy while reducing training data or model size reflect the same property: *strength of the regularity in the data*.






## Comparing concept probing methods
Here we will evaluate the concept probing methods and consider what they tell us about the model. There are numerous challenges related to concept probing, e.g.:

- ***How can we find the best concept probing method?***

When we are evaluating concept probing methods, what we really want to reveal, is how important is a concept to a given model. If the representation of a concept emerges in the hidden representations of a model, then we can infer that the concept is of some usefulness to predict. 

We seek only to reveal what the model has learnt. 

- ***When can we really know if a concept is being used? Even if a concept is linearly separable, how can we know if it is being used?***

Inferring that if a concept is linearly separable then it is being used by the model can sometime be misleading. For example if a concept is linearly separable in the input space, but not useful to the model, it is possible that the model is conserving some non-useful information in the hidden representation. We have not yet quantified this problem.


- ***When is a probing method too simple?*** 

If a probing method is too simple, it may not correctly reveal the degree of which the concept is being used. 

For example, simpler probing methods might not be able to handle problems related to sparsity, e.g. if a concept is most often present under a given condition, which is not a confounding factor, then the probing method may not be able to detect the concept.

From [@DBLP:journals/corr/abs-2111-09259]: "... consider a channel representing the possibility of capturing the opponentâ€™s queen by a positive activation at the position of the queen and zero activations otherwise. There are locations on the board in which the queen is more likely to be captured than other locations, and therefore there are levels of sparsity that lead to positive regression weights only at the positions where the queen is most likely to be captured."

- ***When is a probing method too complex?*** 

Some have argued that it is important to keep the capacity of the regression model low [@DBLP:journals/corr/abs-2111-09259] to ensure that the probing method is capturing the structure of the probed model instead of learning its own structure.

So given one extreme case, where we probe for a concept in the input layer with a very powerful model, the model will learn its own relationships, regardless of the models representations. In the earlier layers of the model, where some non-useful information is still being represented in the hidden layers, i.e. the non-useful information has not been dissolved by the model, the same situation can occur.




### Autoencoder Experiment

This experiment is intended to shed a light on a comparison between different probing architectures. When probing for a concept, it is often hard to know the ground trouth, i.e. to what extend the concept is present. However, if we use an autoencoder, that compresses the input space and successfully reconstructs it, we know that the concepts are somehow represented in the hidden layers of the autoencoder. 

This is relevant when probing a small neural network, such as Stockfish' NNUE, where the concept representations are much more compressed than in a large neural network. 

Here we seek to understand better when a probing method is too simple and giving lower probing accuracy than more powerful probing methods.

*One key insight gained throught this experiment is just how valuable concept probing is to evaluating the reconstruction properties of generative models.* 


#### The Autoencoder

The autoencoder consists of two parts, an encoder and a decoder which are trained to reconstruct the input space. The encoder and decoder mirror each other in terms of the number of layers and the number of neurons in each layer. The design uses a sequence of fully connected linear layers with ReLU activations.

The input into the autoencoder is tensor of size 768, or 64(squares) * 6(piece types) * 2(color).



#### Small Autoencoder
```{python}
#| label: fig-small-autoencoder
#| fig-cap: "Visualizing the results of probing a medium autoencoder."

from stockfish_explain.experiments.chess_auto_encoder.utils import plot_results
import pandas as pd

df_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_small.pkl')
plot_results(df_results)
```



#### Medium Autoencoder
```{python}
#| label: fig-medium-autoencoder
#| fig-cap: "Visualizing the results of probing a medium autoencoder."

from stockfish_explain.experiments.chess_auto_encoder.utils import plot_results
import pandas as pd

df_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_medium.pkl')
plot_results(df_results)
```


#### Large Autoencoder
```{python}
#| label: fig-large-autoencoder
#| fig-cap: "Visualizing the results of probing a large autoencoder."


from stockfish_explain.experiments.chess_auto_encoder.utils import plot_results
import pandas as pd

df_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_large.pkl')
plot_results(df_results)
```


#### Training Experiment 

In order to shed a light on the evolution of probing accuracy, as the autoencoder becomes more powerful, we will probe the autoencoder during training, first after the first epoch, then at a 10 epoch interval.

```{python}
#| label: fig-training-experiment
#| fig-cap: "Visualizing the results of training experiment."


import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_with_dnn.pkl')
model_files = sorted(df_results['model_string'].unique(), key=lambda x: int(x.split('_')[-1].split('.')[0]))
# pivot table df_results with model_string as index and input_name as columns
for feature in df_results['target_name'].unique():
    for model in df_results['model_name'].unique():
        df = df_results[(df_results['target_name'] == feature) & (df_results['model_name'] == model)]
        
        df = df.pivot(index='model_string', columns='input_name', values='score')
        df = df.loc[model_files, ['input_encoder_0', 'encoder_0', 'encoder_1', 'encoder_2','decoder_1','decoder_2','decoder_3']]
        df.index = [x.split('_')[-1].split('.')[0] for x in df.index]
        plt.figure(figsize = (10,10))
        plt.title(f" Accuracy for {feature} - {model}")
        sns.heatmap(df, annot=True, cmap='Blues')       
        plt.xticks([x + 0.5 for x in range(len(df.columns))], ['786', '1024','64','32','64','1024','786'], rotation=45)
        plt.show()
```

#### Plotly experiment
```{python}
#| label: fig-training-experiment2
#| fig-cap: "Visualizing the results of training experiment."

import plotly.graph_objects as go

import pandas as pd

# load dataset
df = pd.read_csv("https://raw.githubusercontent.com/plotly/datasets/master/volcano.csv")

# create figure
fig = go.Figure()

# Add surface trace
fig.add_trace(go.Surface(z=df.values.tolist(), colorscale="Viridis"))

# Update plot sizing
fig.update_layout(
    width=800,
    height=900,
    autosize=False,
    margin=dict(t=0, b=0, l=0, r=0),
    template="plotly_white",
)

# Update 3D scene options
fig.update_scenes(
    aspectratio=dict(x=1, y=1, z=0.7),
    aspectmode="manual"
)

# Add dropdown
fig.update_layout(
    updatemenus=[
        dict(
            buttons=list([
                dict(
                    args=["type", "surface"],
                    label="3D Surface",
                    method="restyle"
                ),
                dict(
                    args=["type", "heatmap"],
                    label="Heatmap",
                    method="restyle"
                )
            ]),
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.1,
            yanchor="top"
        ),
    ]
)

# Add annotation
fig.update_layout(
    annotations=[
        dict(text="Trace type:", showarrow=False,
        x=0, y=1.085, yref="paper", align="left")
    ]
)

fig.show()
```


# References