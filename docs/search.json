[
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Stockfish Explainability",
    "section": "",
    "text": "This document is a work in progress. It is intended to be a guide to the explainability features of Stockfish. It is not intended to be a guide to Stockfish itself. For that, please see the (Stockfish 2022) documentation.\nMeeting notes can be found here\n\n\n\nExplainability is the ability to understand why a machine learning model makes the decisions it does. It is a key component of the Explainable AI movement. It is also a key component of the Fairness, Accountability, and Transparency\n\n\n\nIn the quest to understand why a machine learning model makes the decision it does we want to find out what it has learnt. The methods we will be using to explain Stockfish are:\n\nFeature Importance\nSaliency Maps\nConcept Probing"
  },
  {
    "objectID": "report.html#concepts-from-stockfish",
    "href": "report.html#concepts-from-stockfish",
    "title": "Stockfish Explainability",
    "section": "2.1 Concepts from Stockfish",
    "text": "2.1 Concepts from Stockfish\n\nMaterial\nImbalance\nPawns\nKnights\nBishops\nRooks\nQueens\nMobility\nKing Safety\nThreats\nPassed Pawns\nSpace\nWinnable"
  },
  {
    "objectID": "report.html#implemented-concepts",
    "href": "report.html#implemented-concepts",
    "title": "Stockfish Explainability",
    "section": "2.2 Implemented Concepts",
    "text": "2.2 Implemented Concepts\nAll concepts are implemented for both white and black.\n\nwhite_bishop_pair - Is true if white has a bishop pair\nwhite_knight_pair - Is true if white has a knight pair\nwhite_double_pawn - Is true if white has a double pawn\nwhite_isolated_pawn - Is true if white has an isolated pawn\nwhite_connected_rooks - Is true if white has connected rooks\nwhite_rook_on_open_file - Is true if white has a rook on an open file\nhas_contested_open_file - Is true if there is a contested open file\ncurrent_player_is_forking - Is true if the current player is forking\ncurrent_player_can_fork - Is true if the current player can fork\ncurrent_player_is_checking - Is true if the current player is checking\ncurrent_player_can_check - Is true if the current player can check\nmaterial imbalance - The material imbalance"
  },
  {
    "objectID": "report.html#comparing-concept-probing-methods",
    "href": "report.html#comparing-concept-probing-methods",
    "title": "Stockfish Explainability",
    "section": "3.1 Comparing concept probing methods",
    "text": "3.1 Comparing concept probing methods\nHere we will evaluate the concept probing methods and consider what they tell us about the model. There are numerous challenges related to concept probing, e.g.:\n\nHow can we find the best concept probing method?\n\nWhen we are evaluating concept probing methods, what we really want to reveal, is how important is a concept to a given model. If the representation of a concept emerges in the hidden representations of a model, then we can infer that the concept is of some usefulness to predict.\nWe seek only to reveal what the model has learnt.\n\nWhen can we really know if a concept is being used? Even if a concept is linearly separable, how can we know if it is being used?\n\nInferring that if a concept is linearly separable then it is being used by the model can sometime be misleading. For example if a concept is linearly separable in the input space, but not useful to the model, it is possible that the model is conserving some non-useful information in the hidden representation. We have not yet quantified this problem.\n\nWhen is a probing method too simple?\n\nIf a probing method is too simple, it may not correctly reveal the degree of which the concept is being used.\nFor example, simpler probing methods might not be able to handle problems related to sparsity, e.g. if a concept is most often present under a given condition, which is not a confounding factor, then the probing method may not be able to detect the concept.\nFrom (McGrath et al. 2021): “… consider a channel representing the possibility of capturing the opponent’s queen by a positive activation at the position of the queen and zero activations otherwise. There are locations on the board in which the queen is more likely to be captured than other locations, and therefore there are levels of sparsity that lead to positive regression weights only at the positions where the queen is most likely to be captured.”\n\nWhen is a probing method too complex?\n\nSome have argued that it is important to keep the capacity of the regression model low (McGrath et al. 2021) to ensure that the probing method is capturing the structure of the probed model instead of learning its own structure.\nSo given one extreme case, where we probe for a concept in the input layer with a very powerful model, the model will learn its own relationships, regardless of the models representations. In the earlier layers of the model, where some non-useful information is still being represented in the hidden layers, i.e. the non-useful information has not been dissolved by the model, the same situation can occur.\n\n3.1.1 Autoencoder Experiment\nThis experiment is intended to shed a light on a comparison between different probing architectures. When probing for a concept, it is often hard to know the ground trouth, i.e. to what extend the concept is present. However, if we use an autoencoder, that compresses the input space and successfully reconstructs it, we know that the concepts are somehow represented in the hidden layers of the autoencoder.\nThis is relevant when probing a small neural network, such as Stockfish’ NNUE, where the concept representations are much more compressed than in a large neural network.\nHere we seek to understand better when a probing method is too simple and giving lower probing accuracy than more powerful probing methods.\nOne key insight gained throught this experiment is just how valuable concept probing is to evaluating the reconstruction properties of generative models.\n\n3.1.1.1 The Autoencoder\nThe autoencoder consists of two parts, an encoder and a decoder which are trained to reconstruct the input space. The encoder and decoder mirror each other in terms of the number of layers and the number of neurons in each layer. The design uses a sequence of fully connected linear layers with ReLU activations.\nThe input into the autoencoder is tensor of size 768, or 64(squares) * 6(piece types) * 2(color).\n\n\n3.1.1.2 Small Autoencoder\n\n\n\nCode\nfrom stockfish_explain.experiments.chess_auto_encoder.utils import plot_results\nimport pandas as pd\n\ndf_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_small.pkl')\nplot_results(df_results)\n\n\n\n\n\n(a) Visualizing the results of probing a medium autoencoder.\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n(g)\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n3.1.1.3 Medium Autoencoder\n\n\n\nCode\nfrom stockfish_explain.experiments.chess_auto_encoder.utils import plot_results\nimport pandas as pd\n\ndf_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_medium.pkl')\nplot_results(df_results)\n\n\n\n\n\n(a) Visualizing the results of probing a medium autoencoder.\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n(g)\n\n\n\nFigure 2: ?(caption)\n\n\n\n\n3.1.1.4 Large Autoencoder\n\n\n\nCode\nfrom stockfish_explain.experiments.chess_auto_encoder.utils import plot_results\nimport pandas as pd\n\ndf_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_large.pkl')\nplot_results(df_results)\n\n\n\n\n\n(a) Visualizing the results of probing a large autoencoder.\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n(g)\n\n\n\nFigure 3: ?(caption)"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Meeting Notes",
    "section": "",
    "text": "We went over the following topics:\n\nPaper was rejected, even though the most critical reveiwer increased his evaluation.\nIt only makes sens to experiment with very simple concepts.\nWe talked about the probing problem discussed in the Chess Knowledge paper.\n\nTo evaluate the models mentioned in the paper, we would need to create an experiment.\n\nWe talked about comparing probing methods for two different models.\n\nTODO:\n\nAdd more probing models\nCompare the probing accuracy for the same model at different training steps"
  }
]