[
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Stockfish Explainability",
    "section": "",
    "text": "This document is a work in progress. It is intended to be a guide to the explainability features of Stockfish. It is not intended to be a guide to Stockfish itself. For that, please see the (Stockfish 2022) documentation.\nMeeting notes can be found here\n\n\n\nExplainability is the ability to understand why a machine learning model makes the decisions it does. It is a key component of the Explainable AI movement. It is also a key component of the Fairness, Accountability, and Transparency\n\n\n\nIn the quest to understand why a machine learning model makes the decision it does we want to find out what it has learnt. The methods we will be using to explain Stockfish are:\n\nFeature Importance\nSaliency Maps\nConcept Probing"
  },
  {
    "objectID": "report.html#concepts-from-stockfish",
    "href": "report.html#concepts-from-stockfish",
    "title": "Stockfish Explainability",
    "section": "2.1 Concepts from Stockfish",
    "text": "2.1 Concepts from Stockfish\n\nMaterial\nImbalance\nPawns\nKnights\nBishops\nRooks\nQueens\nMobility\nKing Safety\nThreats\nPassed Pawns\nSpace\nWinnable"
  },
  {
    "objectID": "report.html#implemented-concepts",
    "href": "report.html#implemented-concepts",
    "title": "Stockfish Explainability",
    "section": "2.2 Implemented Concepts",
    "text": "2.2 Implemented Concepts\nAll concepts are implemented for both white and black.\n\nwhite_bishop_pair - Is true if white has a bishop pair\nwhite_knight_pair - Is true if white has a knight pair\nwhite_double_pawn - Is true if white has a double pawn\nwhite_isolated_pawn - Is true if white has an isolated pawn\nwhite_connected_rooks - Is true if white has connected rooks\nwhite_rook_on_open_file - Is true if white has a rook on an open file\nhas_contested_open_file - Is true if there is a contested open file\ncurrent_player_is_forking - Is true if the current player is forking\ncurrent_player_can_fork - Is true if the current player can fork\ncurrent_player_is_checking - Is true if the current player is checking\ncurrent_player_can_check - Is true if the current player can check\nmaterial imbalance - The material imbalance"
  },
  {
    "objectID": "report.html#minimum-description-length-probing",
    "href": "report.html#minimum-description-length-probing",
    "title": "Stockfish Explainability",
    "section": "3.1 Minimum description length probing",
    "text": "3.1 Minimum description length probing\nThe accuracy of a probe is commonly used as a proxy of how well an property is encoded by a model. To show a difference between two properties, often ether the amount of training data or the probing model (size) is restricted.\nInstead this paper presents an approach that approximates the “amount of effort” needed to achieve a given probing accuracy. High accuracy while reducing training data or model size reflect the same property: strength of the regularity in the data."
  },
  {
    "objectID": "report.html#comparing-concept-probing-methods",
    "href": "report.html#comparing-concept-probing-methods",
    "title": "Stockfish Explainability",
    "section": "3.2 Comparing concept probing methods",
    "text": "3.2 Comparing concept probing methods\nHere we will evaluate the concept probing methods and consider what they tell us about the model. There are numerous challenges related to concept probing, e.g.:\n\nHow can we find the best concept probing method?\n\nWhen we are evaluating concept probing methods, what we really want to reveal, is how important is a concept to a given model. If the representation of a concept emerges in the hidden representations of a model, then we can infer that the concept is of some usefulness to predict.\nWe seek only to reveal what the model has learnt.\n\nWhen can we really know if a concept is being used? Even if a concept is linearly separable, how can we know if it is being used?\n\nInferring that if a concept is linearly separable then it is being used by the model can sometime be misleading. For example if a concept is linearly separable in the input space, but not useful to the model, it is possible that the model is conserving some non-useful information in the hidden representation. We have not yet quantified this problem.\n\nWhen is a probing method too simple?\n\nIf a probing method is too simple, it may not correctly reveal the degree of which the concept is being used.\nFor example, simpler probing methods might not be able to handle problems related to sparsity, e.g. if a concept is most often present under a given condition, which is not a confounding factor, then the probing method may not be able to detect the concept.\nFrom (McGrath et al. 2021): “… consider a channel representing the possibility of capturing the opponent’s queen by a positive activation at the position of the queen and zero activations otherwise. There are locations on the board in which the queen is more likely to be captured than other locations, and therefore there are levels of sparsity that lead to positive regression weights only at the positions where the queen is most likely to be captured.”\n\nWhen is a probing method too complex?\n\nSome have argued that it is important to keep the capacity of the regression model low (McGrath et al. 2021) to ensure that the probing method is capturing the structure of the probed model instead of learning its own structure.\nSo given one extreme case, where we probe for a concept in the input layer with a very powerful model, the model will learn its own relationships, regardless of the models representations. In the earlier layers of the model, where some non-useful information is still being represented in the hidden layers, i.e. the non-useful information has not been dissolved by the model, the same situation can occur.\n\n3.2.1 Autoencoder Experiment\nThis experiment is intended to shed a light on a comparison between different probing architectures. When probing for a concept, it is often hard to know the ground trouth, i.e. to what extend the concept is present. However, if we use an autoencoder, that compresses the input space and successfully reconstructs it, we know that the concepts are somehow represented in the hidden layers of the autoencoder.\nThis is relevant when probing a small neural network, such as Stockfish’ NNUE, where the concept representations are much more compressed than in a large neural network.\nHere we seek to understand better when a probing method is too simple and giving lower probing accuracy than more powerful probing methods.\nOne key insight gained throught this experiment is just how valuable concept probing is to evaluating the reconstruction properties of generative models.\n\n3.2.1.1 The Autoencoder\nThe autoencoder consists of two parts, an encoder and a decoder which are trained to reconstruct the input space. The encoder and decoder mirror each other in terms of the number of layers and the number of neurons in each layer. The design uses a sequence of fully connected linear layers with ReLU activations.\nThe input into the autoencoder is tensor of size 768, or 64(squares) * 6(piece types) * 2(color).\n\n\n3.2.1.2 Small Autoencoder\n\n\n\nCode\nfrom stockfish_explain.experiments.chess_auto_encoder.utils import plot_results\nimport pandas as pd\n\ndf_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_small.pkl')\nplot_results(df_results)\n\n\n\n\n\n(a) Visualizing the results of probing a medium autoencoder.\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n(g)\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n3.2.1.3 Medium Autoencoder\n\n\n\nCode\nfrom stockfish_explain.experiments.chess_auto_encoder.utils import plot_results\nimport pandas as pd\n\ndf_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_medium.pkl')\nplot_results(df_results)\n\n\n\n\n\n(a) Visualizing the results of probing a medium autoencoder.\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n(g)\n\n\n\nFigure 2: ?(caption)\n\n\n\n\n3.2.1.4 Large Autoencoder\n\n\n\nCode\nfrom stockfish_explain.experiments.chess_auto_encoder.utils import plot_results\nimport pandas as pd\n\ndf_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_large.pkl')\nplot_results(df_results)\n\n\n\n\n\n(a) Visualizing the results of probing a large autoencoder.\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n(g)\n\n\n\nFigure 3: ?(caption)\n\n\n\n\n3.2.1.5 Training Experiment\nIn order to shed a light on the evolution of probing accuracy, as the autoencoder becomes more powerful, we will probe the autoencoder during training, first after the first epoch, then at a 10 epoch interval.\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndf_results = pd.read_pickle('stockfish_explain/experiments/chess_auto_encoder/data/df_results_with_dnn.pkl')\nmodel_files = sorted(df_results['model_string'].unique(), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n# pivot table df_results with model_string as index and input_name as columns\nfor feature in df_results['target_name'].unique():\n    for model in df_results['model_name'].unique():\n        df = df_results[(df_results['target_name'] == feature) & (df_results['model_name'] == model)]\n        \n        df = df.pivot(index='model_string', columns='input_name', values='score')\n        df = df.loc[model_files, ['input_encoder_0', 'encoder_0', 'encoder_1', 'encoder_2','decoder_1','decoder_2','decoder_3']]\n        df.index = [x.split('_')[-1].split('.')[0] for x in df.index]\n        plt.figure(figsize = (10,10))\n        plt.title(f\" Accuracy for {feature} - {model}\")\n        sns.heatmap(df, annot=True, cmap='Blues')       \n        plt.xticks([x + 0.5 for x in range(len(df.columns))], ['786', '1024','64','32','64','1024','786'], rotation=45)\n        plt.show()\n\n\n\n\n\n(a) Visualizing the results of training experiment.\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n(i)\n\n\n\nFigure 4: ?(caption)"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Meeting Notes",
    "section": "",
    "text": "We went over the following topics:\n\nPaper was rejected, even though the most critical reveiwer increased his evaluation.\nIt only makes sens to experiment with very simple concepts.\nWe talked about the probing problem discussed in the Chess Knowledge paper.\n\nTo evaluate the models mentioned in the paper, we would need to create an experiment.\n\nWe talked about comparing probing methods for two different models.\n\nTODO:\n\nAdd more probing models\nCompare the probing accuracy for the same model at different training steps\n\n\n\n\nWork done:\n\nAdded a neural network probe\nProbed the medium autoencoder during training with three different probing methods\n“The degree of effort” approach reviewed\nImplemented boosting algorithm to approximate degree of effort\n\nDiscussion:\n\nWe saw that a linear probe (and the decision tree probe) showed a decrease in probing accuracy in a compressed state while the network was clearly improving.\nThe neural network probe did not show this behavior.\nThus if we want to use probing to evaluate and compare the quality of a compressed state, we should use a neural network probe.\n\nTODO:\n\nUse a neural network probe (linear and decision tree) to evaluate Stockfish NNUE\nCheck if the boosting algorithm sheds light on concept representation.\n\n\n\n\nWork done:\n\nImported the officiel NNUE weights\nProbed the NNUE at two different depths\nCalculated the probing accuracy for number of classification concepts at both depths\n\nDiscussion:\n\nWe saw that linear probe was often underperforming at layer1 but not layer2\nNo clear pattern was observed for comparison of decision trees and neural network probe\n\nTODO:\n\nImplement binary concept: pawn is on Nth rank\nPerform probing experiment with Regression concepts (stockfish concepts)"
  },
  {
    "objectID": "chess_concepts.html",
    "href": "chess_concepts.html",
    "title": "",
    "section": "",
    "text": "Book: Elements of positional evaluation\n\n\nAspect of a position that do not require analysis of piece movements. E.g. Doubled pawns, open files dark square complex.\n\n\n\nAspect of a position that require knowledge of how the pieces move. It includes both how the pieces move and the time measurement for motion: the tempo\nTempo measurements are the move - one move by each player - and the ply - one move by one player.\n\n\n\nThe part of chess that involves evaluation of the position ( which side stands better, how much and why) and the accompanying planning.\n\n\n\nA forced maneuver that may include winning material, including pawn promotion, and executing a mating attack.\n\nEn prise - a piece that is attacked but not guarded\nCounting - how a player evaluates material trades by the “I take-you take” thought process. Determine whether material can be won by any sequential capturing sequence.\nSingle motifs - examples are double attacks, double theats, pins, removal of the guard, and back rank mates.\nCombinations of motifs without sacrifice\nCombinations of motifs with sacrifice.\n\n\n\n\nA strategy that emphasizes piece and pawn placement, as opposed to tactics."
  },
  {
    "objectID": "chess_concepts.html#static-features-1",
    "href": "chess_concepts.html#static-features-1",
    "title": "",
    "section": "Static Features",
    "text": "Static Features\n\nDoubled Pawns\n\nDoubled pawns ensure at least one semi-open file for the player with the doubled pawns\nBecause doubled pawns result from a pawn capture, at least one of the files adjacent to the doubled pawns has a high probability of being at least semi-open.\nThere is not a high correlation between doubled pawns and either passed or backward pawns.\nIn the endgame doubled pawns are often a liability, as they make creating passed pawns more difficult.\n\nIsolated pawns\n\nThere is much less inherently good, and more bad about isolated pawns than doubled pawns.\n\nBackward Pawns\n\nThe single weakest pawn is the backward pawn on a semi-open file\n\nPassed Pawns\n\nProtected passed pawn - a passed pawn guarded by another pawn\nOutside passed pawn - When both sides have passed pawns, the one farther away from the kings is the “outside” passed pawn\nConnected passed pawns - passed pawns on adjacent files\nThe opponent mush keep an eye on the pawn to prevent queening\nThe opponent should avoid trading into endgames where the passed pawn would give its owner a decisive advantage\nStrong Passed pawn: primarily high mobility, low vulnerability, and a short number of tempi from promotion"
  },
  {
    "objectID": "chess_concepts.html#weak-squares",
    "href": "chess_concepts.html#weak-squares",
    "title": "",
    "section": "Weak Squares",
    "text": "Weak Squares\nhttps://www.youtube.com/watch?v=jEehFvnO3ZA"
  },
  {
    "objectID": "plotly_example.html",
    "href": "plotly_example.html",
    "title": "Plotly Example",
    "section": "",
    "text": "This is an example of a plotly graph using Quarto and Github Pages.\n\n\nCode\nimport plotly.graph_objects as go\n\nimport pandas as pd\n\n# load dataset\ndf = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/volcano.csv\")\n\n# Create figure\nfig = go.Figure()\n\n# Add surface trace\nfig.add_trace(go.Heatmap(z=df.values.tolist(), colorscale=\"Viridis\"))\n\n# Update plot sizing\nfig.update_layout(\n    width=800,\n    height=900,\n    autosize=False,\n    margin=dict(t=100, b=0, l=0, r=0),\n)\n\n# Update 3D scene options\nfig.update_scenes(\n    aspectratio=dict(x=1, y=1, z=0.7),\n    aspectmode=\"manual\"\n)\n\n# Add drowdowns\n# button_layer_1_height = 1.08\nbutton_layer_1_height = 1.12\nbutton_layer_2_height = 1.065\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=list([\n                dict(\n                    args=[\"colorscale\", \"Viridis\"],\n                    label=\"Viridis\",\n                    method=\"restyle\"\n                ),\n                dict(\n                    args=[\"colorscale\", \"Cividis\"],\n                    label=\"Cividis\",\n                    method=\"restyle\"\n                ),\n                dict(\n                    args=[\"colorscale\", \"Blues\"],\n                    label=\"Blues\",\n                    method=\"restyle\"\n                ),\n                dict(\n                    args=[\"colorscale\", \"Greens\"],\n                    label=\"Greens\",\n                    method=\"restyle\"\n                ),\n            ]),\n            type = \"buttons\",\n            direction=\"right\",\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=0.1,\n            xanchor=\"left\",\n            y=button_layer_1_height,\n            yanchor=\"top\"\n        ),\n        dict(\n            buttons=list([\n                dict(\n                    args=[\"reversescale\", False],\n                    label=\"False\",\n                    method=\"restyle\"\n                ),\n                dict(\n                    args=[\"reversescale\", True],\n                    label=\"True\",\n                    method=\"restyle\"\n                )\n            ]),\n            type = \"buttons\",\n            direction=\"right\",\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=0.13,\n            xanchor=\"left\",\n            y=button_layer_2_height,\n            yanchor=\"top\"\n        ),\n        dict(\n            buttons=list([\n                dict(\n                    args=[{\"contours.showlines\": False, \"type\": \"contour\"}],\n                    label=\"Hide lines\",\n                    method=\"restyle\"\n                ),\n                dict(\n                    args=[{\"contours.showlines\": True, \"type\": \"contour\"}],\n                    label=\"Show lines\",\n                    method=\"restyle\"\n                ),\n            ]),\n            type = \"buttons\",\n            direction=\"right\",\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=0.5,\n            xanchor=\"left\",\n            y=button_layer_2_height,\n            yanchor=\"top\"\n        ),\n    ]\n)\n\nfig.update_layout(\n    annotations=[\n        dict(text=\"colorscale\", x=0, xref=\"paper\", y=1.1, yref=\"paper\",\n                             align=\"left\", showarrow=False),\n        dict(text=\"Reverse<br>Colorscale\", x=0, xref=\"paper\", y=1.06,\n                             yref=\"paper\", showarrow=False),\n        dict(text=\"Lines\", x=0.47, xref=\"paper\", y=1.045, yref=\"paper\",\n                             showarrow=False)\n    ])\n\nfig.show()\n\n\n\n\n                                                \nFigure 1: Visualizing the results of training experiment."
  }
]