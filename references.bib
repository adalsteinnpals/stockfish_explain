@article{SilHub18General,
  added-at = {2018-12-13T08:23:43.000+0100},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  biburl = {https://www.bibsonomy.org/bibtex/2cada5fafbda156b2022a6b561174455e/loroch},
  journal = {Science},
  keywords = {AlphaZero RL chess deep_learning deepmind go shogi},
  number = 6419,
  pages = {1140--1144},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2018-12-13T08:23:43.000+0100},
  title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  url = {http://science.sciencemag.org/content/362/6419/1140/tab-pdf},
  volume = 362,
  year = 2018
}

@misc{Stockfish,
  title = {Stockfish},
  author = {Stockfish},
  howpublished = {\url{https://stockfishchess.org/}},
  note = {Accessed: 2022-01-30},
  year = {2022}
}

@misc{Leela,
  title = {Leela Chess Zero},
  author = {LeelaChessZero},
  howpublished = {\url{https://lczero.org/}},
  note = {Accessed: 2022-01-30},
  year = {2022}
}

@misc{DecodeChess,
  title = {Decode Chess},
  author = {Decodea},
  howpublished = {\url{https://decodechess.com/}},
  note = {Accessed: 2022-01-30},
  year = {2022}
}


@misc{DataSets,
  title = {Training Data Sets},
  author = {Stockfish},
  howpublished = {\url{https://github.com/glinscott/nnue-pytorch/wiki/Training-datasets}},
  note = {Accessed: 2022-01-30},
  year = {2022}
}

@article{DBLP:journals/corr/abs-2111-09259,
  author    = {Thomas McGrath and
               Andrei Kapishnikov and
               Nenad Tomasev and
               Adam Pearce and
               Demis Hassabis and
               Been Kim and
               Ulrich Paquet and
               Vladimir Kramnik},
  title     = {Acquisition of Chess Knowledge in AlphaZero},
  journal   = {CoRR},
  volume    = {abs/2111.09259},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09259},
  eprinttype = {arXiv},
  eprint    = {2111.09259},
  timestamp = {Mon, 22 Nov 2021 16:44:07 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09259.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/PuriVGKDK020,
  author    = {Nikaash Puri and
               Sukriti Verma and
               Piyush Gupta and
               Dhruv Kayastha and
               Shripad Deshmukh and
               Balaji Krishnamurthy and
               Sameer Singh},
  title     = {Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SJgzLkBKPB},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/PuriVGKDK020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/AlainB17,
  author    = {Guillaume Alain and
               Yoshua Bengio},
  title     = {Understanding intermediate layers using linear classifier probes},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HJ4-rAVtl},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/AlainB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/ewcbr/Kerner94,
  author    = {Yaakov HaCohen{-}Kerner},
  editor    = {Jean Paul Haton and
               Mark T. Keane and
               Michel Manago},
  title     = {Case-Based Evaluation in Computer Chess},
  booktitle = {Advances in Case-Based Reasoning, Second European Workshop, EWCBR-94,
               Chantilly, France, November 7-10, 1994, Selected Papers},
  series    = {Lecture Notes in Computer Science},
  volume    = {984},
  pages     = {240--254},
  publisher = {Springer},
  year      = {1994},
  url       = {https://doi.org/10.1007/3-540-60364-6\_40},
  doi       = {10.1007/3-540-60364-6\_40},
  timestamp = {Tue, 14 May 2019 10:00:52 +0200},
  biburl    = {https://dblp.org/rec/conf/ewcbr/Kerner94.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/iccbr/Kerner95,
  author    = {Yaakov HaCohen{-}Kerner},
  editor    = {Manuela M. Veloso and
               Agnar Aamodt},
  title     = {Learning Strategies for Explanation Patterns: Basic Game Patterns
               with Application to Chess},
  booktitle = {Case-Based Reasoning Research and Development, First International
               Conference, ICCBR-95, Sesimbra, Portugal, October 23-26, 1995, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {1010},
  pages     = {491--500},
  publisher = {Springer},
  year      = {1995},
  url       = {https://doi.org/10.1007/3-540-60598-3\_45},
  doi       = {10.1007/3-540-60598-3\_45},
  timestamp = {Fri, 27 Dec 2019 21:24:11 +0100},
  biburl    = {https://dblp.org/rec/conf/iccbr/Kerner95.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{conf/csedu/GuidMBSB13,
  title =	"Building an Intelligent Tutoring System for Chess Endgames",
  author =	"Matej Guid and Martin Mozina and Ciril Bohak and
		 Aleksander Sadikov and Ivan Bratko",
  bibdate =	"2017-05-19",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/https://doi.org/10.5220/0004389102630266;
		 DBLP,
		 http://dblp.uni-trier.de/db/conf/csedu/csedu2013.html#GuidMBSB13",
  booktitle =	"CSEDU 2013 - Proceedings of the 5th International
		 Conference on Computer Supported Education, Aachen,
		 Germany, 6-8 May, 2013",
  publisher =	"SciTePress",
  year = 	"2013",
  editor =	"Owen Foley and Maria Teresa Restivo and James Onohuome
		 Uhomoibhi and Markus Helfert",
  ISBN = 	"978-989-8565-53-2",
  pages =	"263--266",
}

@InProceedings{conf/cg/SadikovMGKB06,
  title =	"Automated Chess Tutor",
  author =	"Aleksander Sadikov and Martin Mozina and Matej Guid
		 and Jana Krivec and Ivan Bratko",
  publisher =	"Springer",
  year = 	"2006",
  volume =	"4630",
  bibdate =	"2017-05-17",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/https://doi.org/10.1007/978-3-540-75538-8_2;
		 DBLP,
		 http://dblp.uni-trier.de/db/conf/cg/cg2006.html#SadikovMGKB06",
  booktitle =	"Computers and Games",
  editor =	"H. Jaap van den Herik and Paolo Ciancarini and H. H.
		 L. M. Donkers",
  ISBN = 	"978-3-540-75537-1",
  pages =	"13--25",
  series =	"Lecture Notes in Computer Science",
}



@article{Schwalbe2021,
  author    = {Gesina Schwalbe and
               Bettina Finzel},
  title     = {{XAI} Method Properties: {A} (Meta-)study},
  journal   = {CoRR},
  volume    = {abs/2105.07190},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.07190},
  eprinttype = {arXiv},
  eprint    = {2105.07190},
  timestamp = {Thu, 14 Oct 2021 09:15:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-07190.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{Bodria2021,
  author    = {Francesco Bodria and
               Fosca Giannotti and
               Riccardo Guidotti and
               Francesca Naretto and
               Dino Pedreschi and
               Salvatore Rinzivillo},
  title     = {Benchmarking and Survey of Explanation Methods for Black Box Models},
  journal   = {CoRR},
  volume    = {abs/2102.13076},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.13076},
  eprinttype = {arXiv},
  eprint    = {2102.13076},
  timestamp = {Tue, 02 Mar 2021 12:11:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-13076.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{Ribeiro2016a,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.04938v3},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {arXiv:1602.04938v3},
file = {:home/ap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Why should i trust you Explaining the predictions of any classifier(2).pdf:pdf},
isbn = {9781450342322},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {LIME,saliency},
mendeley-groups = {Papers},
mendeley-tags = {LIME,saliency},
pages = {1135--1144},
title = {{"Why should i trust you?" Explaining the predictions of any classifier}},
volume = {13-17-Augu},
year = {2016}
}

@article{Alvarez-Melis2018,
abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general - explicitness, faithfulness, and stability - and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
archivePrefix = {arXiv},
arxivId = {1806.07538},
author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
eprint = {1806.07538},
file = {:home/ap/Documents/NeurIPS-2018-towards-robust-interpretability-with-self-explaining-neural-networks-Paper.pdf:pdf},
issn = {10495258},
journal = {Nips},
mendeley-groups = {Papers},
number = {NeurIPS},
pages = {7775--7784},
title = {{Towards Robust Interpretability with Self-Explaining Neural Networks}},
volume = {2018-Decem},
year = {2018}
}



@inproceedings{Kim,
abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result-for example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
archivePrefix = {arXiv},
arxivId = {1711.11279},
author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1711.11279},
file = {:home/ap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - Unknown - Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV).pdf:pdf},
isbn = {9781510867963},
keywords = {concept_activation},
mendeley-groups = {Papers},
mendeley-tags = {concept_activation},
pages = {4186--4195},
title = {{Interpretability beyond feature attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
volume = {6},
year = {2018}
}

@misc{nnueintroduction,
  author = {Stockfish},
  title = {introducing-nnue-evaluationo},
  howpublished = {\url{https://blog.stockfishchess.org/post/625828091343896577/introducing-nnue-evaluation}},
  note = {Accessed: 2022-04-05},
  year = {2022}
}

@misc{nnuereadme,
  author = {Stockfish},
  title = {nnue readme},
  howpublished = {\url{https://github.com/glinscott/nnue-pytorch/blob/master/docs/nnue.md}},
  note = {Accessed: 2022-04-05},
  year = {2022}
}

@article{Mi2020,
abstract = {In recent years, black-box models have developed rapidly because of their high accuracy. Balancing the interpretability and accuracy is increasingly important. The lack of interpretability severely limits the application of the model in academia and industry. Despite the various interpretable machine learning methods, the perspective and meaning of the interpretation are also different. We provide a review of the current interpretable methods and divide them based on the model being applied. We divide them into two categories: interpretable methods with the self-explanatory model and interpretable methods with external co-explanation. And the interpretable methods with external co-explanation are further divided into subbranch methods based on instances, SHAP, knowledge graph, deep learning, and clustering model. The classification aims to help us understand the model characteristics applied in the interpretable method better. This survey makes the researcher find a suitable model to solve interpretability problems more easily. And the comparison experiments contribute to discovering complementary features from different methods. At the same time, we explore the future challenges and trends of interpretable machine learning to promote the development of interpretable machine learning.},
author = {Mi, Jian Xun and Li, An Di and Zhou, Li Fang},
doi = {10.1109/ACCESS.2020.3032756},
file = {:home/ap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mi, Li, Zhou - 2020 - Review study of interpretation methods for future interpretable machine learning.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Deep learning,Influential instance,Interpretable machine learning,Interpretable methods with external co-explanation,Interpretable methods with the selfexplanatory mod,Knowledge graph,survey},
mendeley-groups = {Papers},
mendeley-tags = {survey},
pages = {191969--191985},
title = {{Review study of interpretation methods for future interpretable machine learning}},
volume = {8},
year = {2020}
}

@inproceedings{Lundberg,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M and Lee, Su In},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1705.07874},
file = {:home/ap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Allen, Lee - Unknown - A Unified Approach to Interpreting Model Predictions(2).pdf:pdf},
issn = {10495258},
keywords = {SHAP,saliency,shapley},
mendeley-groups = {Papers},
mendeley-tags = {SHAP,saliency,shapley},
pages = {4766--4775},
title = {{A unified approach to interpreting model predictions}},
url = {https://github.com/slundberg/shap},
volume = {2017-Decem},
year = {2017}
}




@article{Fritz2021,
abstract = {State-of-the-art game playing programs do not provide an explanation for their move choice beyond a numerical score for the best and alternative moves. However, human players want to understand the reasons why a certain move is considered to be the best. Saliency maps are a useful general technique for this purpose, because they allow visualizing relevant aspects of a given input to the produced output. While such saliency maps are commonly used in the field of image classification, their adaptation to chess engines like Stockfish or Leela has not yet seen much work. This paper takes one such approach, Specific and Relevant Feature Attribution (SARFA), which has previously been proposed for a variety of game settings, and analyzes it specifically for the game of chess. In particular, we investigate differences with respect to its use with different chess engines, to different types of positions (tactical vs. positional as well as middle-game vs. endgame), and point out some of the approach's down-sides. Ultimately, we also suggest and evaluate a few improvements of the basic algorithm that are able to address some of the found shortcomings.},
author = {Fritz, Jessica and F{\"{u}}rnkranz, Johannes},
file = {:home/ap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fritz, F{\"{u}}rnkranz - 2021 - Some Chess-Specific Improvements for Perturbation-Based Saliency Maps(2).pdf:pdf},
isbn = {9781665438865},
journal = {IEEE Conference on Games},
keywords = {Chess,Index Terms-Explainable AI,Machine learning,chess,saliency},
mendeley-groups = {Papers},
mendeley-tags = {chess,saliency},
title = {{Some Chess-Specific Improvements for Perturbation-Based Saliency Maps}},
url = {http://www.cegt.net/rating.htm},
year = {2021}
}


@article{APYB2021,
  title={Evaluating Interpretability Methods for DNNs in Game-Playing Agents},
  journal = {ACG Advances in Computer Games},
  year = {2021},
  author={P{\'a}lsson, A{\dh}alsteinn}
}






@article{Silver2017,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {1712.01815},
file = {:Users/adalsteinnpalsson/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.pdf:pdf},
month = {dec},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
url = {http://arxiv.org/abs/1712.01815},
year = {2017}
}


@misc{lightgbm,
  title = {LightGBM},
  author = {Microsoft Corporation},
  howpublished = {\url{https://lightgbm.readthedocs.io/en/latest/}},
  note = {Accessed: 2022-01-30},
  year = {2022}
}


@article{lipovetsky2001analysis,
  title={Analysis of regression in game theory approach},
  author={Lipovetsky, Stan and Conklin, Michael},
  journal={Applied Stochastic Models in Business and Industry},
  volume={17},
  number={4},
  pages={319--330},
  year={2001},
  publisher={Wiley Online Library}
}

@article{CASTRO20091726,
title = {Polynomial calculation of the Shapley value based on sampling},
journal = {Computers \& Operations Research},
volume = {36},
number = {5},
pages = {1726-1730},
year = {2009},
note = {Selected papers presented at the Tenth International Symposium on Locational Decisions (ISOLDE X)},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2008.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305054808000804},
author = {Javier Castro and Daniel Gómez and Juan Tejada},
keywords = {Game theory, Shapley value, Sampling algorithm},
abstract = {In this paper we develop a polynomial method based on sampling theory that can be used to estimate the Shapley value (or any semivalue) for cooperative games. Besides analyzing the complexity problem, we examine some desirable statistical properties of the proposed approach and provide some computational results.}
}







